{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 安裝必要的套件",
   "id": "cb0e3ecf66e955d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install jieba --quiet\n",
    "!pip install tensorflow --quiet\n",
    "!pip install pandas --quiet"
   ],
   "id": "c48ce6f2d9de7c83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 步驟 1: 上傳檔案",
   "id": "88e31c237f256919"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()  # 選擇上傳 Cleaned_Positive_Words.xlsx 和 Negative_Words_List.xlsx"
   ],
   "id": "4ade4172c74d989e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Main Code",
   "id": "8e27cba46a31cf1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import packages",
   "id": "53833c711d15f7f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T11:19:56.695681Z",
     "start_time": "2025-02-01T11:19:56.690184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 導入必要模組\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.src.models import Sequential\n",
    "from keras.src.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "4fbdcfbdbc404e1e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### define path",
   "id": "c73d71c6991881c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T11:37:36.795856Z",
     "start_time": "2025-02-01T11:37:36.781191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = Path(\"./data/emtion_analyse/emotion_analyse_dataset.csv\")\n",
    "data_frame = pd.read_csv(dataset_path)\n",
    "positive_words = data_frame[data_frame['is_positive']].dropna()\n",
    "negative_words = data_frame[~data_frame['is_positive']].dropna()"
   ],
   "id": "7069ee0fd17ec47f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T12:15:25.108283Z",
     "start_time": "2025-02-01T12:15:24.953596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 步驟 2: 設置檔案路徑\n",
    "positive_data_path = './data/emtion_analyse/positive_words.xlsx'\n",
    "negative_data_path = './data/emtion_analyse/negative_words.xlsx'\n",
    "\n",
    "# 步驟 3: 讀取資料\n",
    "positive_reviews = pd.read_excel(positive_data_path, header=None)\n",
    "negative_reviews = pd.read_excel(negative_data_path, header=None)\n",
    "\n",
    "# 提取正面和負面詞彙\n",
    "positive_words = positive_reviews[1][1:].dropna()  # 跳過標題行並移除空值\n",
    "negative_words = negative_reviews[1][1:].dropna()"
   ],
   "id": "26d16d7d1be3419f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-01T12:15:27.485062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# 構造影評資料集\n",
    "positive_samples = [\"這是一個非常\" + word + \"的產品，值得推薦！\" for word in positive_words]\n",
    "negative_samples = [\"這是一個非常\" + word + \"的產品，完全不推薦！\" for word in negative_words]\n",
    "\n",
    "# 創建標籤\n",
    "positive_labels = [1] * len(positive_samples)  # 正面為1\n",
    "negative_labels = [0] * len(negative_samples)  # 負面為0\n",
    "\n",
    "# 合併影評與標籤\n",
    "texts = positive_samples + negative_samples\n",
    "labels = positive_labels + negative_labels\n",
    "\n",
    "# 步驟 4: 定義分詞函數\n",
    "def preprocess_texts(texts):\n",
    "    return [\" \".join(jieba.lcut(text)) for text in texts]\n",
    "\n",
    "# 分詞處理影評\n",
    "texts = preprocess_texts(texts)\n",
    "\n",
    "# 步驟 5: 文本編碼與數據處理\n",
    "num_words = 5000  # 詞彙表大小\n",
    "maxlen = 100      # 每條影評的最大長度\n",
    "\n",
    "# 使用 Tokenizer 將影評轉為數字序列\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(texts)  # 建立詞彙表\n",
    "sequences = tokenizer.texts_to_sequences(texts)  # 將影評轉為數字序列\n",
    "x_data = pad_sequences(sequences, maxlen=maxlen)  # 填充序列\n",
    "y_data = np.array(labels)  # 標籤\n",
    "\n",
    "# 拆分訓練集和測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# 步驟 6: 建立 LSTM 模型\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=num_words, output_dim=64, input_length=maxlen),  # 嵌入層\n",
    "    LSTM(units=128, return_sequences=False),  # LSTM 層\n",
    "    Dropout(0.5),  # Dropout 防止過擬合\n",
    "    Dense(units=1, activation='sigmoid')  # 輸出層\n",
    "])\n",
    "\n",
    "# 編譯模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
   ],
   "id": "f71b3ba678bcae10",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.893 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/home/siorimiya/miniconda3/envs/thesis_for_zt/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738412129.396045    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738412129.744254    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738412129.744420    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-01 20:15:29.744460: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2432] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 5.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1738412129.751383    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738412129.751635    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738412129.751726    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-01 20:15:29.751743: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2432] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 5.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1738412130.072166    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1738412130.072297    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-01 20:15:30.072310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1738412130.072385    2389 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-01 20:15:30.072413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1457 MB memory:  -> device: 0, name: NVIDIA GeForce MX130, pci bus id: 0000:01:00.0, compute capability: 5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 步驟 7: 測試模型\n",
    "sample_review = \"這是一個非常感人的產品，值得推薦！\"  # 測試用影評\n",
    "sample_review = \" \".join(jieba.lcut(sample_review))  # 分詞\n",
    "sample_sequence = tokenizer.texts_to_sequences([sample_review])  # 轉為數字序列\n",
    "sample_padded = pad_sequences(sample_sequence, maxlen=maxlen)  # 填充序列"
   ],
   "id": "a34831a43a1ed29b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 預測結果\n",
    "prediction = model.predict(sample_padded)\n",
    "print(\"Prediction (Positive Sentiment Probability):\", prediction[0][0])\n",
    "\n",
    "# 結果解釋\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\"這是一條正面評價！\")\n",
    "else:\n",
    "    print(\"這是一條負面評價！\")"
   ],
   "id": "2e47eca4ece37b18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
